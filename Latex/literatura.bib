@article{BK07,
author = {Bell, Robert M. and Koren, Yehuda},
title = {Lessons from the Netflix Prize Challenge},
year = {2007},
issue_date = {December 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1931-0145},
url = {https://doi.org/10.1145/1345448.1345465},
doi = {10.1145/1345448.1345465},
abstract = {This article outlines the overall strategy and summarizes a few key innovations of the team that won the first Netflix progress prize.},
journal = {SIGKDD Explor. Newsl.},
month = {dec},
pages = {75â€“79},
numpages = {5}
}

@article{CR08,
  author    = {Emmanuel J. Cand{\`{e}}s and
               Benjamin Recht},
  title     = {Exact Matrix Completion via Convex Optimization},
  journal   = {CoRR},
  volume    = {abs/0805.4471},
  year      = {2008},
  url       = {http://arxiv.org/abs/0805.4471},
  eprinttype = {arXiv},
  eprint    = {0805.4471},
  timestamp = {Mon, 13 Aug 2018 16:47:57 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-0805-4471.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@online{VV20,
  author = {Barry Van Veen},
  title = {Matrix Completion},
  year = 2020,
  url = {https://www.youtube.com/watch?v=lS0FVKJ4Xfg},
  urldate = {2023-03-12}
}

@book{D97,
  added-at = {2016-10-21T03:01:06.000+0200},
  author = {Demmel, James W.},
  biburl = {https://www.bibsonomy.org/bibtex/2e173b16b7e9211fdff422d592920739c/ytyoun},
  doi = {10.1137/1.9781611971446},
  interhash = {14289df57fa476cd6c2092037ceeee57},
  intrahash = {e173b16b7e9211fdff422d592920739c},
  keywords = {courant-fischer eigenvalues linear.algebra matrix numerical.analysis textbook},
  month = jan,
  publisher = {SIAM},
  timestamp = {2016-10-21T03:01:06.000+0200},
  title = {Applied Numerical Linear Algebra},
  year = 1997
}

@article{SeDuMi,
  author = {Sturm, Jos F.},
  title = {Using SeDuMi 1.02, A {MATLAB} toolbox for optimization over symmetric cones},
  journal = {Optimization Methods and Software},
  volume = {11},
  number = {1-4},
  pages = {625-653},
  year  = {1999},
  doi = {10.1080/10556789908805766},
  URL = {https://doi.org/10.1080/10556789908805766}
}

@misc{CCS,
      title={A Singular Value Thresholding Algorithm for Matrix Completion}, 
      author={Jian-Feng Cai and Emmanuel J. Candes and Zuowei Shen},
      year={2008},
      eprint={0810.3286},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@ARTICLE{HZYLH12,
  author={Hu, Yao and Zhang, Debing and Ye, Jieping and Li, Xuelong and He, Xiaofei},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Fast and Accurate Matrix Completion via Truncated Nuclear Norm Regularization}, 
  year={2013},
  volume={35},
  number={9},
  pages={2117-2130},
  doi={10.1109/TPAMI.2012.271}}

@article{AST-TK15,
title = {Low rank matrix completion by alternating steepest descent methods},
journal = {Applied and Computational Harmonic Analysis},
volume = {40},
number = {2},
pages = {417-429},
year = {2016},
issn = {1063-5203},
doi = {https://doi.org/10.1016/j.acha.2015.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S1063520315001062},
author = {Jared Tanner and Ke Wei},
keywords = {Matrix completion, Alternating minimization, Gradient descent, Exact line-search},
abstract = {Matrix completion involves recovering a matrix from a subset of its entries by utilizing interdependency between the entries, typically through low rank structure. Despite matrix completion requiring the global solution of a non-convex objective, there are many computationally efficient algorithms which are effective for a broad class of matrices. In this paper, we introduce an alternating steepest descent algorithm (ASD) and a scaled variant, ScaledASD, for the fixed-rank matrix completion problem. Empirical evaluation of ASD and ScaledASD on both image inpainting and random problems show they are competitive with other state-of-the-art matrix completion algorithms in terms of recoverable rank and overall computational time. In particular, their low per iteration computational complexity makes ASD and ScaledASD efficient for large size problems, especially when computing the solutions to moderate accuracy such as in the presence of model misfit, noise, and/or as an initialization strategy for higher order methods. A preliminary convergence analysis is also presented.}
}

@article{LMaFit-WY12,
author = {Wen, Zaiwen and Yin, Wotao and Zhang, Yin},
year = {2012},
month = {12},
pages = {},
title = {Solving a low-rank factorization model for matrix completion by a nonlinear successive over-relaxation algorithm},
volume = {4},
journal = {Mathematical Programming Computation},
doi = {10.1007/s12532-012-0044-1}
}

@phdthesis{NNM-PHD,
    title        = {Matrix rank minimization with applications},
    author       = {Maryam Fazel},
    year         = 2002,
    month        = {March},
    address      = {Stanford, CA},
    school       = {Stanford University},
    type         = {PhD dezertacija}
}

@ARTICLE{Survey-NKS19,
  author={Nguyen, Luong Trung and Kim, Junhan and Shim, Byonghyo},
  journal={IEEE Access}, 
  title={Low-Rank Matrix Completion: A Contemporary Survey}, 
  year={2019},
  volume={7},
  number={},
  pages={94215-94237},
  doi={10.1109/ACCESS.2019.2928130}}