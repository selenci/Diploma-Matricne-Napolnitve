\section{Minimizacija prirezane nuklearne norme}
Že samo ime nam pove, da bo algoritem minimizacije prirezane nuklearne norme, oziroma TNNM (Truncated Nuclear Norm Minimization) podoben algoritmu NNM. Vendar tu privzamemo, da imamo o samem algoritmu še neko dodatno informacijo $r \in \mathbb{N}$, ki je povezana z rangom originalne, nezašumljene matrike.

Sam algoritem uvede tako imenovano skrajšano nuklearno normo, ki je za matriko $X \in \mathbb{R}^{n_1 \times n_2}$ definirana kot
\[
    ||X||_r = \sum^{\min(n_1, n_2)}_{i = r + 1} \delta_i(X)
\] torej vsoto $\min(n_1,n_2) - r$ najmanjših singularnih vrednosti, ter z 
njeno pomočjo definira problem \cite{TNNM-HZYLH12}
\begin{align*}
    \min_X & \hspace{0.5cm} ||X||_r \\
    \textrm{tako da} & \hspace{0.5cm} \proj(X) = \proj(M)
\end{align*}


Cilj algoritma je torej čim bolj zmanjšati najmanjše singularne vrednosti, medtem ko velikih ne omejujemo. S tem problem minimizacije omilimo.

Za reševanje minimizacije bomo uporabljali algoritem ADMM, vendar moramo prej sam problem nekoliko spremeniti. Razpišimo problem kot 
\begin{align*}
    \min_X & \hspace{0.5cm} ||X||_* - \sum_{i=1}^{r} \sigma_i \\
    \textrm{tako da} & \hspace{0.5cm} \proj(X) = \proj(M)
\end{align*}.

Za nadaljne korake bomo potrebovali teorem
\[
    \tr(AXB^T) \leq \sum_{i=1}^{r}
\]
kjer velja $X \in \mathbb{R}^{n_1 \times n_2}$, $A \in \mathbb{R}^{r \times n_1}$, $B \in \mathbb{R}^{r \times n_2}$ in $r \in \mathbb{N}, r \leq min(n_1, n_2)$, kot tudi $AA^T = I_{r}, BB^T = I_{r}$.

Za dokaz uporabljamo Von Neumannovo neenakost sledi, s katero lahko zapišemo 
\[
    \tr(AXB^T) = \tr(XB^TA) \leq \sum^{\min(n_1, n_2)}_{i=1} \sigma_i(X) \sigma_i(B^TA)
\]
Enakost $\tr(AXB^T) = \tr(XB^TA)$ sledi iz dejstva, da je sled produkta matrik invariantna pod cikličnimi permutacijami.

Po definiciji lahko singularne vrednosti matrike $Y$ najdemo tako, da najdemo korene nenegativnih lastnih vrednosti matrike $Y^TY$. Tako lahko povemo, da so singularne vrednosti matrike $B^TA$ enake lastnim vrednostim matrike $A^TBB^TA$. Izraz razpišemo v
\[
A^TBB^TA = A^TI_rA = A^TA
\]
%https://shorturl.at/pqrRX
Ker pa velja, da imata matriki $XY$ in $YX$ enake neničelne lastne vrednosti, ter vemo da $AA^T = I_r$, lahko povemo, da ima produkt $B^TA$ $r$ singularnih vrednosti enakih 1, saj ima $I_n$ $n$ lastnih vrednosti enakih 1.
Tako lahko sedaj razpišemo izraz 
\[
    \sum^{\min(n_1, n_2)}_{i=1} \sigma_i(X) \sigma_i(B^TA) = \sum^{r}_{i=1} \sigma_i(X)
\]
Ugotovili smo, da velja 
\begin{equation}
    \label{2806-1320}
    \tr(AXB^T) \leq \sum^{r}_{i=1} \sigma_i(Xs)
\end{equation}


Če definiramo SVD razcep $X = U \Sigma V^T$, ter matriki 
$A = (u_1, \hdots , u_r)^T$ in $B = (v_1, \hdots , v_r)^T$,
kjer je $u_i$ $i$-ti stolpec matrike $U$ ter $v_i$ $i$-ti stolpec 
matrike $V$, lahko pokažemo da velja $\tr(AXB^T) = \sum^{r}_{i=1} \sigma_i(X)$.
\begin{align*}
    \tr(AXB^T) &= \tr((u_1, \hdots , u_r)^T X (u_1, \hdots , u_r)^T) = \\
    &= \tr((u_1, \hdots , u_r)^T U \Sigma V^T (u_1, \hdots , u_r)^T) = \\
    &= \begin{bmatrix} I_r & 0 \\ 0 & 0 \end{bmatrix} \Sigma \begin{bmatrix} I_r & 0 \\ 0 & 0 \end{bmatrix} = \\
    &= \tr(\begin{bmatrix} \sigma_1 \\[-8pt] & \ddots & & \\[-8pt] & & \sigma_r \\[-8pt] & &  & 0 \\[-8pt]  & & & & \ddots \\[-8pt] & & & & & 0 \end{bmatrix}) = \sum_{i = 1}^{r} \sigma_i(X)  \numberthis\label{2806-1321}
\end{align*}
Z združitvijo dokazov \ref{2806-1320} in \ref{2806-1321} zapišemo
\[
    \max_{AA^T = I, BB^T = I} \tr(AXB^T) = \sum^{r}_{i = 1} \sigma_i(X)
\]
Torej je problem, ki ga minimizarmo lahko podan kot 
\begin{align*}
    \min_X \hspace{0.5cm} &||X||_* - \max_{AA^T = I, BB^T = I} \tr(AXB^T)\\
    \text{tako da} \hspace{0.5cm} &\proj(X) = \proj(M)
\end{align*}

Glede na vse ugotovitve, nato nastavimo iterativni algoritem, tako da, izračunamo $X^0 = \proj(M)$. V $i$-ti iteraciji izračunamo $A^i$ in $B^i$, tako da izračunamo SVD razcep $X^i = U \Sigma V^T$, ter $A$ nastavimo kot prvih $r$ stolpcev matrike $U$, $B$ pa kot prvih $r$ stolpcev matrike $V$. $X^{i+1}$ lahko sedaj izračunamo kot \cite{TNNM-HZYLH12}
\begin{align*}
    \min_X \hspace{0.5cm} &||X||_* - \tr(A^iX(B^i)^T)\\
    \text{tako da} \hspace{0.5cm} &\proj(X) = \proj(M)
\end{align*}
Če minimizacijo še malo obrnemo, pa lahko problem rešujemo z uporabo algoritma ADMM. \todo{iz kje pride izracun $Y^{i+1}$} 
\begin{align*}
    \min_X \hspace{0.5cm} &||X||_* - \tr(A^iW(B^i)^T)\\
    \text{tako da} \hspace{0.5cm} &W=X, \hspace{0.2cm} \proj(W) = \proj(M)
\end{align*}
Ta problem ponovno zapišemo s pomočjo Lagrangeove funkcije, le da algoritem ADMM definira še kazenski parameter $\beta$. \cite{TNNM-HZYLH12} \todo{kazenski parameter?}
\[ 
\mathcal{L}(X, Y, W, \beta) = ||X||_* - Tr(A_l W B_l^T) + \frac{\beta}{2} ||X - W||_F^2 + Tr(Y^T(X-W))
\]
Opazimo lahko, da funkcija definira zgolj pogoj $X = W$. Videli bomo, da algoritem pogoj $\proj(X) = \proj(M)$ definira posredno, s popravljanjem znanih vrednosti. \ref{2906-1248}

Matriko $X^{k+1}$ ponovno definiramo kot 
\[
  X^{k+1} = \arg \min_X \mathcal{L}(X, Y^k, W^k, \beta)  
\]
Z ignoriranjem konstantnih členov, pa lahko tako zapišemo
\begin{align*}
  X^{k+1} &= \arg \min_X ||X||_* + \frac{\beta}{2}\trOp{X-W^k}{X-W^k} + \frac{\beta}{2}\trOp{\frac{2}{\beta}Y}{X} = \\
  &= \arg \min_X ||X||_* + \frac{\beta}{2}\tr(X^TX - X^TW^k - W^{k^T}X - W^{k^T}W^k + \frac{2}{\beta}Y^{k^T}X )
\end{align*}
Z namenom faktorizacije dodamo konstantne člene.
\begin{align*}
    &\arg \min_X ||X||_* + \frac{\beta}{2}\tr(X^T(X - W^k + \frac{1}{\beta}Y^k) - W^{k^T} (X - W^k + \frac{1}{\beta}Y^k)\\
    &+\frac{1}{\beta}Y^{k^T}(X - W^k + \frac{1}{\beta}Y^k)) =\\ 
  &= \arg \min_X ||X||_* + \frac{\beta}{2} ||X-W^k + \frac{1}{\beta}Y^k||^2_F = \\
  &= \arg \min_X \frac{1}{\beta}||X||_* +  \frac{1}{2}||X-(W^k - \frac{1}{\beta}Y^k)||^2_F
\end{align*}
Po teoremu \ref{2906-1056} pa tako lahko zapišemo 
\[
    X^{k+1} = \shrink_\frac{1}{\beta}(W_k - \frac{1}{\beta} Y_k)
\]
Matriko $W$ podobno izračunamo kot 
\begin{align*}
    W^{k+1} &= \arg \min_{W} \mathcal{L}(X^{k+1}, Y^k, W, \beta) \\ 
    &= \arg \min_W \frac{\beta}{2} ||W - (X^{k+1} + \frac{1}{\beta}(A_l^T B_l + Y_k)) ||^2_F
\end{align*}
Lahko je videti, da velja
\[
    W^{k+1} = X^{k+1} + \frac{1}{\beta}(A_l^T B_l + Y_k)
\]
zaradi pogoja $\proj(W) = \proj(M)$ pa tiste elemente, ki poznamo, popravimo. \cite{TNNM-HZYLH12}\todo{malo spremenjeno iz clanka}
\begin{equation}
    \label{2906-1248}
    W_{k+1} = W_{k+1} + \proj(M - W_{k+1})
\end{equation}
