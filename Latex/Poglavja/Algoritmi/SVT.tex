\section{Algoritem praga singularnih vrednosti}\label{2807-1441}
\textbf{Algoritem praga singularnih vrednosti (SVT)} \cite{CCS} uporabi idejo, da imajo matrike z majhnim rangom nekaj velikih singularnih vrednosti, ostale pa 0 ali pa vsaj blizu 0. Ključna parametra v SVT-ju sta \textit{izbira premika} in \textit{izbira praga},  
%Za svoje delovanje uvede dva nova pomembna koncepta, prvi je premik, drugi pa prag, potreben za uporabo operatorja $\shrink_\tau$ \eqref{1007-1959}. 
algoritem pa temelji na iteraciji
\begin{align}
\label{2407-1910}
        X^{(k)} &= \shrink_\tau(Y^{(k-1)}), \\
        Y^{(k)} &= Y^{(k-1)} + \delta_k \proj(M - X^{(k)}), 
\end{align}
kjer so $\tau > 0$ izbran prag, $\delta_k$ izbran premik, $X^{(0)} = 0 \in \mathbb{R}^{n_1 \times n_2}$ in
$Y^{(0)} = 0 \in \mathbb{R}^{n_1 \times n_2}$. 

V nadaljevanju bomo opisali glavno idejo zgornje iteracije. V grobem pa temelji na uporabi metode za iskanje vezanih ekstremov, kjer elementi matrik $Y^{(k)}$ predstavljajo Lagrangeove množitelje. 

Uvedimo funkcijo 
\begin{align}
    \label{1007-2007}
    f_\tau(X) = \tau\nnorm{X} + \frac{1}{2}\fnorm{X}^2
\end{align}
in optimizacijski problem
\begin{align}
\label{2706-0957}
\begin{split}
    \min_{X\in \mathbb R^{n_1\times n_2}} & \hspace{0.5cm} f_\tau(X), \\
    \textrm{pri pogojih} & \hspace{0.5cm} \proj(X) = \proj(M).
\end{split}
\end{align}
Opazimo lahko, da za velike vrednosti $\tau$ velja $f_\tau(X) \approx \tau\nnorm{X}$, kar pomeni, da bo s primerno izbranim $\tau$, optimizacijski problem \eqref{2706-0957} minimiziral nuklearno normo.

Denimo, da želimo poiskati minimum funkcije $f(x)$ pri pogojih $g_1(x)=g_2(x)=\ldots=g_{k}(x)=0$.
V teoriji vezanih ekstremov se za tovrstne probleme uvede \textbf{Lagrangeovo funkcijo}
\[\mathcal{L}(x, \lambda_1,\lambda_2,\ldots,\lambda_k) = f(x) + \lambda_1 g_1(x)+\lambda_2g_2(x)+\ldots+\lambda_kg_k(x),\]
nato pa išče ekstreme med njenimi stacionarnimi točkami.
Problemu \eqref{2706-0957} lahko priredimo Lagrangeovo funkcijo
\[
    \mathcal{L}(X, Y) = f_\tau(X) + \left< Y, \proj(M - X) \right>,
\] 
nato pa iščemo njene stacionarne točke.
Zaradi velikega števila parametrov pa ta pristop navadno ni izvedljiv, zato se v SVT-ju za iskanje ekstremov $\mathcal{L}(X, Y)$ uporabi t.i.\ \textit{Uzawa algoritem} \cite{CCS}. Ta ekstreme išče prek iterativnega postopka:
\begin{align}
        X^{(k)} &=  \argmin_{X} \mathcal{L}(X^{(k)}, Y^{(k-1)}) \label{1007-2018},\\
        Y^{(k)} &= Y^{(k-1)} + \delta_k \proj(M - X^{(k)}). \label{1007-2019}
\end{align}

Izkaže se, da je rešitev \eqref{1007-2018} enaka $\shrink_\tau(Y^{(k-1)})$. To spodaj dokažemo, še prej pa izpeljimo pomožen rezultat.

\begin{trditev}
Velja:
\begin{align}
    \argmin_X  \mathcal{L}(X, Y)
    = \argmin_X \Big(\tau\nnorm{X} + \frac{1}{2}\fnorm{X - Y}^2\Big)
\end{align}
\end{trditev}

\begin{proof}
Trditev sledi iz naslednjega računa:
\begin{align*}
    &\argmin_X  \Big(\tau\nnorm{X} + \frac{1}{2}\fnorm{X - Y}^2\Big) \\
    &=^1\argmin_X \Big(\tau\nnorm{X} + \frac{1}{2}\trOp{X - Y}{X - Y}\Big)\\
    &=^2\argmin_X \Big(\tau\nnorm{X} + \frac{1}{2}(\fnorm{X}^2 - 2\trOp{X}{Y} + \fnorm{Y}^2)\Big) \\ 
    &=^3\argmin_X \Big(\tau\nnorm{X} + \frac{1}{2}\fnorm{X}^2 - \trOp{X} {\proj(Y)}\Big)\\
    &=^4\argmin_X \Big(\tau\nnorm{X} + \frac{1}{2}\fnorm{X}^2 + \tr(- \proj(X)Y^T) + \tr(\proj(M)Y^T)\Big)\\
    &=^5\argmin_X \Big(\tau\nnorm{X} + \frac{1}{2}\fnorm{X}^2 + \tr(\proj(M - X)Y^T)\Big)\\
    &=^6\argmin_X \Big(\tau\nnorm{X} + \frac{1}{2}\fnorm{X}^2 + \trOp{Y}{\proj(M - X)}\Big)\\ 
    &=^7 \argmin_X \mathcal{L}(X, Y)
\end{align*}
kjer smo v prvi enakosti uporabili definicijo Frobeniusove norme, v drugi bilinearnost skalarnega produkta, 
v tretji smo ignorirali konstanto $\fnorm{Y}^2$, saj ne vpliva na rezultat, 
in upoštevali $\proj(Y^{(k)}) = Y^{(k)}$ za vse $k \in \mathbb{N}$. Zadnje dejstvo sledi iz definicije \eqref{1007-2019} in $Y^{(0)} = 0$. V četrti enakosti smo upoštevali $\trOp{X}{\proj(Y)} = \trOp{\proj(X)}{Y}$, kar je lahko videti, ko se spomnimo, da za skalarni produkt $\trOp{A}{B}$ matrik $A, B \in \mathbb{R}^{n_1 \times n_2}$ velja 
\[
    \trOp{A}{B} = \sum_{i = 1}^{n_1}\sum_{j = 1}^{n_2} a_{ij}b_{ij}.
\]
Prišteli smo tudi konstanto $\tr(\proj(M)Y^T)$, ki ne vpliva na rezultat.
V peti enakosti smo upoštevali linearnost sledi in aditivnost operatorja $P_\Omega$,
v šesti definicijo skalarnega produkta in v zadnji definicijo Lagrangeove funkcije.
\end{proof}
\begin{theorem} \label{1907-2240}
Za matriki $X \in \mathbb{R}^{n_1 \times n_2}$ in 
$Y \in \mathbb{R}^{n_1 \times n_2}$ velja:
\begin{align}
    \label{2906-1056}
    \shrink_\tau(Y) = \argmin_{X} \Big(\frac{1}{2} \fnorm{X-Y}^2 + \tau\nnorm{X} \Big) 
\end{align}
\end{theorem}

\begin{proof} 
Najprej se spomnimo definicije konveksne funkcije. Funkcija $f$ je konveksna, če za katerikoli dve točki $x_1, x_2$ v domeni funkcije $f$ velja, da je premica čez ti dve točki, na odseku med tema dvema točkama, nad grafom funkcije $f$.
Funkcija $h(X) := \frac{1}{2} \fnorm{X-Y}^2 + \tau\nnorm{X} $ je konveksna funkcija, saj potreben pogoj trikotniške neenakosti matričnih norm zagotavlja, da je matrična norma konveksna funkcija. Vsota konveksnih funkcij pa je prav tako konveksna funkcija. 
%\todo{ali je potrebno citirati} 
Zaradi konveksnosti funkcije $f$ je 
subgradient v vsaki točki iz domene dobro definiran.
Tega smo definirali v razdelku \ref{2607-1502}.
Matrika $Z\in \mathbb R^{n_1\times n_2}$ je subgradient funkcije $f$ v točki 
$X_0\in \mathbb R^{n_1\times n_2}$,
če velja 
\[\forall X\in \mathbb R^{n_1\times n_2}: 
f(X) \geq  f(X_0) + \trOp{Z}{X - X_0}.\]
Iz definicije subgradienta sledi, da bo imela funkcija $f$ minimum v točki $X'$ natanko tedaj,
ko bo ničelna matrika $\mathbf{0}$ eden izmed subgradientov funkcije $f$ v točki $X_0$.
\iffalse 
V minimumu funkcije $f$ bo eden od subgradientov iščemo minimum $X'$ funkcije $f$,  iščemo tako točko $X'$, da bo
\CG{eden izmed subgradientov po spremenljivki $X$} v točki $X'$ enak 0.  
\fi 

Izkaže se \cite{CCS}, da je množica subgradientov nuklearne norme v točki $X$ enaka
\begin{equation}  \label{2807-0932}  
    \partial\nnorm{X} = \{UV^T + W: W \in \mathbb{R}^{n_1 \times n_2}, U^TW = \mathbf{0}, WV = \mathbf{0}, \norm{W}_2 \leq 1 \},
\end{equation}
kjer $U \Sigma V^T$ predstavlja SVD razcep matrike $X$. S krajšim računom lahko preverimo, da za vsak par $(i,j)$ velja $\frac{\partial}{\partial X_{ij}}\fnorm{X-Y}^2 = 2(X_{ij} - Y_{ij})$. 
Če te parcialne odvode zložimo v matriko, dobimo $2(X-Y)$. Od tod sledi, da je $X-Y$ eden od subgradientov funkcije $h_1(X):=\frac{1}{2}\fnorm{X-Y}^2$ v točki $X$ \cite[3.1.3]{boyd2004convex}.
Iz teh dveh premislekov sledi, da bo $X'$ minimum $h$ natanko tedaj, ko velja 
\begin{equation*}
    \mathbf{0} \in X' - Y + \tau \partial\nnorm{X'}.
\end{equation*}
Trditev izreka bo sledila, če pokažemo, da velja 
\begin{equation*} 
    \mathbf{0} \in \shrink_\tau(Y) - Y + \tau \partial\nnorm{\shrink_\tau(Y)},
\end{equation*}
oz.\ ekvivalentno
\begin{equation} \label{0508-1746}
    Y-\shrink_\tau(Y) \in \tau \partial\nnorm{\shrink_\tau(Y)},
\end{equation}
Sedaj razčlenimo SVD razcep matrike $Y$ kot 
\begin{equation} \label{0508-1752}
    Y = U_0\Sigma_0V_0^T + U_1\Sigma_1V_1^T,
\end{equation}
kjer se $U_0, \Sigma_0$ in $V_0$ nanašajo na singularne vrednosti in pripadajoče singularne vektorje, večje od $\tau$, $U_1, \Sigma_1$ in $V_1$ pa na tiste, manjše od $\tau$. 
Spomnimo se definicije 
\begin{equation} \label{0508-1757}
\shrink_\tau(Y)=U_0(\Sigma_0 - \tau I)V_0^T.
\end{equation}
Upoštevamo \eqref{0508-1752} in \eqref{0508-1757}  v \eqref{0508-1746}, s čimer preostane dokazati
\begin{equation} \label{0508-1755}
    U_1\Sigma_1V_1^T+\tau U_0 V_0^T\in
    \tau \partial\nnorm{\shrink_\tau(Y)}.
\end{equation}
\iffalse
Pokazati želimo, da velja 
\[
    X' = U_0(\Sigma_0 - \tau I)V_0^T,
\] 
kar pa je po definiciji natanko enako operatorju $\shrink_\tau(Y)$.
S pretvorbo \eqref{2807-0940} pridemo do zapisa
\begin{equation} \label{2807-0934}
    Y- X' \in \tau \partial \nnorm{X'}.
\end{equation}
\fi
Sedaj iščemo tako matriko $W$, da bodo zanjo držali pogoji, podani v \eqref{2807-0932}, uporabljeni za $X=\shrink_\tau(Y)$, prav tako pa bo po veljala enakost
\begin{equation} \label{0508-1804}
    U_1\Sigma_1V_1^T+\tau U_0 V_0^T
    =
    \tau(U_0V_0^T +W).
\end{equation}
V zadnji enakosti smo upoštevali, da je SVD razcep matrike $\shrink_\tau(Y)$ enak \eqref{0508-1757}.
Definirajmo
\begin{equation} \label{0508-1807}
    W= \tau^{-1} U_1 \Sigma_1 V_1^T
\end{equation}
in preverimo, da je to prava izbira za $W$.
\iffalse
Veljavnost željene enakosti
preverimo s kratkima računoma:
\begin{align*}
    Y-X' &= U_0\Sigma_0V_0^T + U_1\Sigma_1V_1^T - U_0(\Sigma_0 - \tau I)V_0^T \\ 
    &= U_0(\Sigma_0 - \Sigma_0 + \tau I)V_0^T + U_1\Sigma_1 V_1^T  \\
    &= \tau U_0 V_0^T + U_1\Sigma_1 V_1^T
\end{align*}
in 
\fi
Če upoštevamo \eqref{0508-1807} na desni strani \eqref{0508-1804}, dobimo
\begin{align*}
    \tau(U_0 V_0^T + W) &= \tau(U_0V_0^T + \tau^{-1} U_1 \Sigma_1 V_1^T)\\ 
    &= \tau U_0 V_0^T + U_1 \Sigma_1 V_1^T, 
\end{align*}
kar je ravno leva stran \eqref{0508-1804}.
Sedaj je zgolj potrebno pokazati, da veljajo potrebne lastnosti matrike $W$ iz \eqref{2807-0932}. 
Iz definicije matrik $U_0,U_1,V_0,V_1$ sledi $U_0^T U_1=V_1^TV_0=0$. Torej veljata enakosti $U_0^TW = 0$ in $WV_0 = 0$. Ker pa ima matrika $\Sigma_1$ vse elemente manjše od $\tau$, velja tudi $\norm{W}_2 \leq 1$. $\norm{W}_2$ je namreč enaka največji singularni vrednosti matrike $W$, ki jo razberemo iz diagonale matrike $\tau^{-1}\Sigma_1$. S tem smo preverili veljavnost \eqref{0508-1746}, kar konča dokaz izreka.
\end{proof}

Z uporabo \eqref{1007-2018}, \eqref{1007-2019}
in izreka \ref{1907-2240}
res pridemo do iteracije
\eqref{2407-1910}, ki jo uporablja algoritem SVT.
\iffalse
Tako res pridemo 
Po trditvi lahko sedaj zapišemo algoritem \eqref{1007-2018} - \eqref{1007-2019} kot \cite{CCS}
\[
    \begin{cases}
        X^k = \shrink_\tau(Y^{k-1}) \\
        Y^k = Y^{k-1} + \delta_k \proj(M - X^k) 
    \end{cases}
\]
\fi

\subsection{Nastavljanje parametrov $\tau$ in $\delta$} \label{1907-1648}
Opazimo lahko, da algoritem SVT potrebuje dva parametra, $\tau$ in $\delta$, ki ju moramo izbrati 
že pred vstopom v algoritem.
Po priporočilih \cite{CCS}
sta primerni izbiri za $\delta$ in $\tau$
enaki 
\[
    \delta = 1.2\, \dfrac{n_1 n_2}{m}\qquad\text{in}\qquad
    \tau = 5n,
\]
pri čemer je $\tau$ naveden za kvadratne $n\times n$ matrike.
V poglavju z rezultati smo prvotno uporabljali ti dve konstanti, pri 
čemer smo zaradi
pravokotnosti $n_1\times n_2$ matrik uporabljali
$\tau = 5\frac{n_1+n_2}{2}$.
\iffalse
Medtem, ko so koraki v samem algoritmu definirani kot množica korakov, 
smo v okviru rezultatov diplomske naloge, prvotno za premik uporabljali konstanto, ter korak nastavili na 
 po priporočilih \cite{CCS}. 
 \fi
\iffalse
Prav tako članek \cite{CCS} navaja, da je za matrike velikosti $\mathbb{R}^{n \times n}$ smiselno nastaviti $\tau = 5n$, vendar sem v moji implementaciji zaradi posploševanja na nekvadratne matrike, za matrike velikosti $\mathbb{R}^{n_1 \times n_2}$ parameter nastavil na
\[
    \tau = 5\, \frac{n_1+n_2}{2}
\]
\fi
V naših testiranjih se je izkazalo, da sta tako izbrana parametra dobri izbiri za večje matrike. V razdelku \ref{1307-2251} pa bomo videli, da moramo pri manjših matrikah pogosto zmanjšati premik in povečati prag.