\section{Prag singularnih vrednosti}
Algoritem praga singularnih vrednosti, oziroma v nadaljevanju SVT uporabi idejo, da imamo pri matrikah z majhnim rangom nekaj velikih singularnih vrednosti, ostale pa blizu 0. Za svoje delovanje uvede dva nova pomembna koncepta, prvi je premik, drugi pa prag, potreben za uporabo operatorja $\shrink_\tau$ \eqref{1007-1959}. Algoritem temelji na iteraciji
\[
    \begin{cases}
        X^k = \shrink_\tau(Y^{k-1}) \\
        Y^k = Y^{k-1} + \delta_k \proj(M - X^k) 
    \end{cases}
\]
kjer je $\tau > 0$ izbran prag, $\delta_k$ izbran premik, $X^0 = 0 \in \mathbb{R}^{n_1 \times n_2}$ ter
$Y^0 = 0 \in \mathbb{R}^{n_1 \times n_2}$. \cite{CCS}

V nadaljevanju bomo opisali glavno idejo zgornje iteracije. V grobem pa temelji na uporabi metode za iskanje vezanih ekstremov, kjer elementi matrik $Y^k$ predstavljajo Lagrangove množitelje. 

Uvedimo funkcijo 
\begin{align}
    \label{1007-2007}
    f_\tau(X) = \tau\nnorm{X} + \frac{1}{2}\fnorm{X}^2
\end{align}
Problem lahko tako zapišemo kot 
\begin{align}
\label{2706-0957}
\begin{split}
    \textrm{min} & \hspace{0.5cm} f_\tau(X) \\
    \textrm{pri pogojih} & \hspace{0.5cm} \proj(X) = \proj(M)
\end{split}
\end{align}
Opazimo lahko, da za velike vrednosti $\tau$ velja $f_\tau(X) \approx \tau\nnorm{X}$, kar pomeni, da bo s primerno izbranim $\tau$, algoritem res minimiziral nuklearno normo.

Definirajmo Lagrangeovo funkcijo 
\[\mathcal{L}(x, \lambda) = f(x) + \lambda \cdot g(x),\]
kjer je $f(x)$ \eqref{2706-0957} funkcija, ki jo minimiziramo, pod pogojem, da velja $g(x) = 0$. Naš problem tako prevedemo v
\[
    \mathcal{L}(X, Y) = f_\tau(X) + \left< Y, \proj(M - X) \right>
\] 
\todo{Je ta citat smiselen?}
S pomočjo tako imenovanega Uzawoega algoritma \cite{CCS}, pa lahko problem pretvorimo v iterativni algoritem. 
\begin{align}
        X^k =  \arg \min_{X} \mathcal{L}(X^k, Y^{k-1}) \label{1007-2018}\\
        Y^k = Y^{k-1} + \delta_k \proj(M - X^k) \label{1007-2019}
\end{align}
Izkaže se, da je rešitev \eqref{1007-2018} enaka $\shrink_\tau(Y^{k-1})$.

\todo{Poglej SVT clanek 2.14}
\begin{theorem}
\begin{align}
    \label{2906-1056}
    \shrink_\tau(Y) = \arg \min_{X} \{ \frac{1}{2} \fnorm{X-Y}^2 + \tau\nnorm{X} \}
\end{align}
\end{theorem}

\begin{proof}
Ker je $h(X) := \frac{1}{2} \fnorm{X-Y}^2 + \tau\nnorm{X} $ strogo konveksna funkcija, lahko za subgradient $Z$ v točki $X_0$ rečemo, da velja $\forall X: f(X) \geq  f(X_0) + \left< Z, X - X_0 \right>$. Ali drugače povedano, vse točke na vseh \CR{tangentah} na funckijo $h(X)$ bodo pod ali na funkciji $h(X)$. To velja po sami definiciji, saj je to zahtevan pogoj subgradienta v neki točki.

Pri iskanju minimuma torej iščemo tako točko $X'$, da bo subgradient v točki $X'$ enak 0. Problem sedaj zapišemo kot $0 \in X' - Y + \tau \partial\nnorm{X'}$. Izkaže se, da je množica subgradientov nuklearne norme definirana kot
\[
    \partial\nnorm{X} = \{UV^* + W: W \in \mathbb{R}^{n_1 \times n_2}, U^*W = 0, WV = 0, \norm{W}_2 \leq 1 \}.
\]
kjer $U \Sigma V^T$ predstavlja SVD razcep matrike X. \cite{CCS}

Cilj dokaza je pokazati, da velja $X' = \shrink_\tau(Y)$ Najprej razčlenimo SVD razcep matrike $Y$ kot 
\[
    Y = U_0\Sigma_0V_0^T + U_1\Sigma_1V_1^T,s
\]
kjer $U_0, \Sigma_0$ in $V_0$ predstavljajo lastne vrednosti ter njihove lastne vektorje večje od $\tau$, $U_1, \Sigma_1$ in $V_1$ pa tiste manjše od $\tau$. Ustrezno je torej pokazati, da velja 
\[
    X' = U_0(\Sigma_0 - \tau I)V_0^T
\] Gre preprosto za drugačen zapis operatorja $\shrink_\tau(Y)$.
Če zapis vstavimo v prejšnji podan pogoj dobimo
\begin{align*}
    0 = X' - Y + \tau \partial \nnorm{X}\\
    Y- X' = \tau (U_0 V_0^T + W)
\end{align*}
primerna izbira za $W = \tau^{-1} U_1 \Sigma_1 V_1^T$, saj
\begin{align*}
    Y-X' &= U_0\Sigma_0V_0^T + U_1\Sigma_1V_1^T - U_0(\Sigma_0 - \tau I)V_0^T =\\ 
    &= U_0V_0^T(\Sigma_0 - \Sigma_0 + \tau I) + U_1\Sigma_1 V_1^T = \\
    &= \tau U_0 V_0^T + U_1\Sigma_1 V_1^T
\end{align*}
in 
\begin{align*}
    \tau(U_0 V_0^T + W) &= \tau(U_0V_0^T + \tau^{-1} U_1 \Sigma_1 V_1^T)\\ 
    &= \tau U_0 V_0^T + U_1 \Sigma_1 V_1^T 
\end{align*}

Sedaj je zgolj potrebno pokazati, da veljajo potrebne lastnosti matrike $W$.
Po sami definiciji SVD vemo, da so vsi stolpci matrik U in V ortogonalni. Torej velja $U_0^TW = 0$ in $WV_0 = 0$. Ker pa ima matrika $\Sigma_1$ vse elemente manjše od $\tau$ velja tudi $\norm{W}_2 \leq 1$. S tem smo pokazali, da $Y - X' \in \tau \partial \nnorm{X'}$.
\end{proof}

Po trditvi lahko sedaj zapišemo algoritem \eqref{1007-2018} - \eqref{1007-2019} kot \cite{CCS}
\[
    \begin{cases}
        X^k = \shrink_\tau(Y^{k-1}) \\
        Y^k = Y^{k-1} + \delta_k \proj(M - X^k) 
    \end{cases}
\]

\subsection{Nastavljanje parametrov $\tau$ in $\delta$}
Opazimo lahko, da algoritem SVT potrebuje dva parametra, $\tau$ in $\delta$, ki ju moramo izbrati v naprej.

Medtem, ko so koraki v samem algoritmu definirani kot množica korakov, 
smo v okviru rezultatov diplomske naloge, \CG{prvotno} za premik uporabljali konstanto, ter korak nastavili na 
\[
    \delta = 1.2\, \dfrac{n_1 n_2}{m}
\] po priporočilih \cite{CCS}. 

Prav tako članek \cite{CCS} navaja, da je za matrike velikosti $\mathbb{R}^{n \times n}$ smiselno nastaviti $\tau = 5n$, vendar sem v moji implementaciji zaradi posploševanja na nekvadratne matrike, za matrike velikosti $\mathbb{R}^{n_1 \times n_2}$ parameter nastavil na
\[
    \tau = 5\, \frac{n_1+n_2}{2}
\]

Medtem, ko so taki parametri dobri za večje matrike, jih ne smemo uporabljati kot definitivno najboljše vrednosti. Med mojimi testiranji sem ugotovil, da je vrednost premika velikokrat treba zmanjšati, posebno za manjše matrike z več neznanimi vrednostmi, saj drugače program ni konvergiral. Prav tako, se je večkrat zgodilo, da je bil pridobljen rezultat še vedno zelo zašumljen. Takrat je bilo smiselno prag $\tau$ povečati. To je sicer upočasnilo program, vendar izboljšalo rezultat.