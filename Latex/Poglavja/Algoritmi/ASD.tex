\section{Izmenjajoč gradientni spust}
Ker je računanje SVD razcepa zahtevna operacija, saj ima časovno zahtevnost
$O(n^3)$, je bilo predlaganih nekaj algoritmov, ki za svoje delovanje ne potrebujejo SVD-ja. Algoritem Izmenjajočega gradientnega spusta, oziroma v nadaljevnu ASD (Alternating Steepest Descent) sloni na računanju gradienta in premikanju po njem. Glavni cilj algoritma je, najti dve matriki $X \in \mathbb{R}^{n_1 \times r}$ ter $Y \in \mathbb{R}^{r \times n_2}$, tako da bo veljalo $M = XY$. Vidimo lahko, da ponovno potrebujemo informacijo o rangu matrike, ki jo rekonstruiramo. Ker imata tako $X$ in $Y$ kvečjemu rang $r$, vemo, da tudi njun produkt $XY$ ne bo imel ranga večjega od $r$. 

Cilj algoritma je minimizirati \todo{zakaj 1/2}
\[
    \min_{X, Y} \hspace{0.5cm} \frac{1}{2}\, ||\proj(M) - \proj(XY)||^2_F
\] 
Algoritem minimizacijo razdeli na dve, nato pa izmenično rešuje eno in nato drugo kot 
\begin{align*}
    X_{i+1} &= \arg \min_{X} \hspace{0.3cm} ||\proj(M) - \proj(XY_i)||^2_F \\
    Y_{i+1} &= \arg \min_{Y} \hspace{0.3cm} ||\proj(M) - \proj(X_{i+1}Y)||^2_F
\end{align*}
\cite{AST-TK15}

Za uporabo algoritma ASD potrebujemo odvoda funkcije $f(X,Y) = \frac{1}{2}\, ||\proj(M) - \proj(XY)||^2_F$, ki ga izračunamo za vsak element posebej.
\todo{Zakaj pride $Y^T$ \href{https://math.stackexchange.com/questions/2128462/gradient-of-squared-frobenius-norm-of-a-matrix}{math stackexchange}} 
\begin{align*}
    \frac{\partial}{\partial x_{a,b}} f &= \frac{\partial}{\partial x_{a,b}} \frac{1}{2}\, ||\proj(M) - \proj(XY)||^2_F = \\
    &= \frac{\partial}{\partial x_{a,b}} \frac{1}{2}\, \sum_{i}^{n_1}\sum_{j}^{n_2}(\delta_{i,j}m_{i,j} - \delta_{i,j}\sum_{k}^{r}(x_{i,k}y_{k,j}))^2 = \\
    &= \, \sum_{j}^{n_2}(\delta_{a,j}m_{a,j} - \delta_{i,j}\sum_{k}^{r}(x_{a,k}y_{k,j}))(-y_{b,j}) \implies \\
    &\implies \frac{\partial}{\partial X}f = -(\proj(M) - \proj(XY))Y^T
    % &= \sum_{i,j}(\delta_{i,j}m_{i,j} - \delta_{i,j}\sum_{k}(x_{i,k}y_{k,j}))
    % (\sum_{k}y_{k,j}) = \\
    % &= (P(M) - P(XY))Y^T
\end{align*}
kjer 
\[
    \delta_{i,j} = \begin{cases}
        1, (i, j) \in \Omega \\
        0, (i, j) \notin \Omega
    \end{cases}
\]

Na podoben način bi lahko pokazali tudi
\[
    \frac{\partial}{\partial Y} = -X^T (\proj(M) - \proj(XY))
\]

\todo{Kako pokazati korak}