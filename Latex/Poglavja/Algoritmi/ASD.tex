\section{Izmenjujoč gradientni spust} \label{2707-1337}
Računanje SVD razcepa je zahtevna operacija, saj ima časovno zahtevnost
$O(n^3)$, kar je lahko za veliko matrike zelo počasno. Zato je bilo izpeljanih nekaj algoritmov, ki za svoje delovanje ne potrebujejo SVD-ja. \textbf{Algoritem izmenjujočega gradientnega spusta (ASD)} \cite{AST-TK15} 
temelji na izračunu gradienta in premiku
v smeri tega za nek korak. Ideja algoritma je poiskati matriki $X \in \mathbb{R}^{n_1 \times r}$ ter $Y \in \mathbb{R}^{r \times n_2}$, tako da velja $\proj(M) = \proj(XY)$. Tudi tu potrebujemo informacijo o rangu matrike, ki jo rekonstruiramo. Ker imata $X$ in $Y$ rang največ $r$, tudi njun produkt $XY$ ne bo imel ranga večjega od $r$. 

Cilj algoritma je rešiti optimizacijski problem
\[
    \min_{\substack{X\in \mathbb{R}^{n_1 \times r}, \\Y \in \mathbb{R}^{r \times n_2}}} \hspace{0.5cm} \frac{1}{2}\, \fnorm{\proj(M) - \proj(XY)}^2
\] 
Algoritem problem minimizacije razdeli na dva optimizacijska podproblema, ki ju rešuje izmenično s pomočjo iterativnega postopka
\begin{align}
\label{2507-0927}
    X^{(k+1)} &= \argmin_{X} \fnorm{\proj(M) - \proj(XY^{(k)})}^2, \\
\label{2507-0928}
    Y^{(k+1)} &= \argmin_{Y} \fnorm{\proj(M) - \proj(X^{(k+1)}Y)}^2
\end{align}


Za uporabo algoritma ASD potrebujemo gradienta funkcije $f(X,Y) = \frac{1}{2}\, \fnorm{\proj(M) - \proj(XY)}^2$ po obeh spremenljivkah, ki ju izračunamo za vsak element posebej.
Za reševanje \eqref{2507-0927} so spremenljivke elementi matrike $X$,
za reševanje \eqref{2507-0928} pa elementi matrike $Y$. Zaradi enostavnosti definiramo dve pomožni funkciji. Funkcija $f_Y(X)$ je enaka funkciji $f(X, Y)$, le da vrednost $Y$ jemljemo kot konstanto. Simetrično definiramo $f_X(Y)$, kjer jemljemo vrednost $X$ funkcije $f(X, Y)$ za konstantno. Sedaj preprosto poiščemo oba gradienta pomožnih funkcij, označena z $\nabla f_Y(X)$ in $\nabla f_X(Y)$ ter najboljša premika po gradientih, označena s $t_x$ in $t_y$. Sam algoritem se potem preprosto premika po gradientih za določen premik. Koraka algoritma bomo v vsaki iteraciji definirali kot
\begin{align*}
    X^{(k+1)} = X^{(k)} - t_{x^{(k)}} \nabla f_{Y^{(k)}}(X^{(k)}) \numberthis \label{2607-1726},\\
    Y^{(k+1)} = Y^{(k)} - t_{y^{(k)}} \nabla f_{X^{(k+1)}}(Y^{(k)}).
\end{align*}

\begin{theorem}
Velja
\begin{align*}
    \nabla f_Y(X) &= -(\proj(M) - \proj(XY))Y^T,\\
    \nabla f_X(Y) &= -X^T (\proj(M) - \proj(XY)),\\
    t_x &= \frac{\fnorm{\nabla f_Y(X)}^2}{\fnorm{\proj(\nabla f_Y(X)Y)}^2},  \\
    t_y &= \frac{\fnorm{\nabla f_X(Y)}^2}{\fnorm{\proj(X \nabla f_X(Y))}^2}. 
\end{align*}
\end{theorem}

\begin{proof}
\CG{Pri tem dokazu} nam bo prav prišla \CG{Diracova $\Omega$ -} $\delta_{i, j}$, definirana kot
\[
    \delta_{i,j} = 
    \left\{
    \begin{array}{rl}
        1,& (i, j) \in \Omega, \\
        0,& (i, j) \notin \Omega.
    \end{array}
    \right.
\]
Za gradient funkcije $f_Y(X)$ po spremenljivki X, označen z $\nabla f_Y(X)$, velja
\begin{align*}
    \nabla f_Y(X)_{a,b} &= \frac{\partial}{\partial x_{a,b}} \frac{1}{2}\, \fnorm{\proj(M) - \proj(XY)}^2  \\
    &= \frac{\partial}{\partial x_{a,b}} \frac{1}{2}\, \sum_{i=1}^{n_1}\sum_{j=1}^{n_2}(\delta_{i,j}m_{i,j} - \delta_{i,j}\sum_{k=1}^{r}(x_{i,k}y_{k,j}))^2  \\
    &= \, \sum_{j=1}^{n_2}(\delta_{a,j}m_{a,j} - \delta_{i,j}\sum_{k=1}^{r}(x_{a,k}y_{k,j}))(-y_{b,j}) \\
    \implies &\nabla f_Y(X) = -(\proj(M) - \proj(XY))Y^T
    % &= \sum_{i,j}(\delta_{i,j}m_{i,j} - \delta_{i,j}\sum_{k}(x_{i,k}y_{k,j}))
    % (\sum_{k}y_{k,j}) = \\
    % &= (P(M) - P(XY))Y^T
\end{align*}

Na podoben način lahko pokažemo še
\[
    \nabla f_X(Y) = -X^T (\proj(M) - \proj(XY))
\]
Poiščimo še najboljši korak gradientnega spusta.
Cilj je pokazati, da je premik po gradientu s korakom $t_x$ najboljši.
\CG{Sam premik smo zgoraj \eqref{2607-1726} definirali} kot
\[
    X^{(k+1)} = X^{(k)} - t_{x^{(k)}} \nabla f_{Y^{(k)}}(X^{(k)})
\]
Ker želimo, da bodo znane vrednosti produkta $X^{(k+1)}Y^{(k)}$ kar se da podobne znanim vrednostim matrike $M$, nastavimo $t_x$ kot 
\begin{align*}
    t_x &= \argmin_t g(t),
\end{align*} kjer je
\begin{align*}
    g(t) &= \frac{1}{2} \fnorm{\proj(M) - \proj((X - t\nabla f_Y(X))Y)}^2.
\end{align*}
Kot v \cite{AST-TK15} se izkaže, da velja
\[
  g'(t) = -\fnorm{\nabla f_Y(X)}^2 + t\fnorm{\proj(f_Y(X)Y)}^2. 
\]
Od tod sklepamo, da funkcija $g(t)$ doseže minimum pri 
\[
  t_x = \frac{\fnorm{\nabla f_Y(X)}^2}{\fnorm{\proj(\nabla f_Y(X)Y)}^2}  
\]
Podobno velja za korak v smeri gradientnega spusta matrike $Y$, kjer
\[
  t_y = \frac{\fnorm{\nabla f_X(Y)}^2}{\fnorm{\proj(X \nabla f_X(Y))}^2}  
\]

\end{proof}