\section{Izmenjajoč gradientni spust}
Ker je računanje SVD razcepa zahtevna operacija, saj ima časovno zahtevnost
$O(n^3)$, je bilo predlaganih nekaj algoritmov, ki za svoje delovanje ne potrebujejo SVD-ja. Algoritem Izmenjajočega gradientnega spusta, oziroma v nadaljevnu ASD sloni na računanju gradienta in premikanja po njem. Ideja algoritma je najti dve matriki $X \in \mathbb{R}^{n_1 \times r}$ ter $Y \in \mathbb{R}^{r \times n_2}$, tako da velja $\proj(M) = \proj(XY)$. Vidimo lahko, da ponovno potrebujemo informacijo o rangu matrike, ki jo rekonstruiramo. Ker imata tako $X$ in $Y$ kvečjemu rang $r$, vemo, da tudi njun produkt $XY$ ne bo imel ranga večjega od $r$. 

Cilj algoritma je minimizirati
\[
    \min_{X, Y} \hspace{0.5cm} \frac{1}{2}\, \fnorm{\proj(M) - \proj(XY)}^2
\] 
Algoritem minimizacijo razdeli na dve, nato pa izmenično rešuje eno in nato drugo kot \cite{AST-TK15}
\begin{align*}
    X_{i+1} &= \arg \min_{X} \hspace{0.3cm} \fnorm{\proj(M) - \proj(XY_i)}^2 \\
    Y_{i+1} &= \arg \min_{Y} \hspace{0.3cm} \fnorm{\proj(M) - \proj(X_{i+1}Y)}^2
\end{align*}


Za uporabo algoritma ASD potrebujemo odvoda funkcije $f(X,Y) = \frac{1}{2}\, \fnorm{\proj(M) - \proj(XY)}^2$, ki ga izračunamo za vsak element posebej.
\begin{align*}
    \frac{\partial}{\partial x_{a,b}} f &= \frac{\partial}{\partial x_{a,b}} \frac{1}{2}\, \fnorm{\proj(M) - \proj(XY)}^2  \\
    &= \frac{\partial}{\partial x_{a,b}} \frac{1}{2}\, \sum_{i}^{n_1}\sum_{j}^{n_2}(\delta_{i,j}m_{i,j} - \delta_{i,j}\sum_{k}^{r}(x_{i,k}y_{k,j}))^2  \\
    &= \, \sum_{j}^{n_2}(\delta_{a,j}m_{a,j} - \delta_{i,j}\sum_{k}^{r}(x_{a,k}y_{k,j}))(-y_{b,j}) \implies \\
    &\implies \frac{\partial}{\partial X}f = -(\proj(M) - \proj(XY))Y^T
    % &= \sum_{i,j}(\delta_{i,j}m_{i,j} - \delta_{i,j}\sum_{k}(x_{i,k}y_{k,j}))
    % (\sum_{k}y_{k,j}) = \\
    % &= (P(M) - P(XY))Y^T
\end{align*}
kjer 
\[
    \delta_{i,j} = \begin{cases}
        1, (i, j) \in \Omega \\
        0, (i, j) \notin \Omega
    \end{cases}
\]
Na podoben način bi lahko pokazali tudi
\[
    \frac{\partial}{\partial Y} = -X^T (\proj(M) - \proj(XY))
\]
Poiščimo še najboljši korak gradientnega spusta.
Cilj je pokazati, da je premik po gradientu s korakom $t_x$ najboljši.
Najprej definirajmo sam premik, kot
\[
    X^{k+1} = X^k - t_X \nabla f_Y(X)
\]
Ker želimo, da bodo znane vrednosti produkta $X^{k+1}Y^{k}$ kar se da podobne znanim vrednostim matrike $M$, nastavimo $t_x$ kot 
\begin{align*}
    t_x &= \arg \min_t \hspace{0.3cm} g(t)
\end{align*} kjer
\begin{align*}
    g(t) &= \frac{1}{2} \fnorm{\proj(M) - \proj((X - t\nabla f_Y(X))Y)}^2 \\
    g(t) &= \frac{1}{2} \fnorm{\proj(M - XY)}^2 - t\tr(\proj(M - XY)\proj(\nabla f_Y(X)Y)^T) + \frac{t^2}{2}\fnorm{\proj(\nabla f_Y(X)Y)}^2
\end{align*}
Ker je \cite{AST-TK15} \todo{ali lahko tako?}
\[
  g'(t) = -\fnorm{\nabla f_Y(X)}^2 + t\fnorm{\proj(f_Y(X)Y)}^2 
\]
vidimo, da funkcija $g(t)$ doseže minimum pri 
\[
  t_x = \frac{\fnorm{\nabla f_Y(X)}^2}{\fnorm{\proj(\nabla f_Y(X)Y)}^2}  
\]
Podobno velja za korak v smeri gradientnega spusta matrike $Y$, kjer
\[
  t_y = \frac{\fnorm{\nabla f_X(Y)}^2}{\fnorm{\proj(X \nabla f_X(Y))}^2}  
\]